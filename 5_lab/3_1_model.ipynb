{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import uuid\n",
    "import os\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из-за того, что мы создаем датасет с помощью матчинга негативных текстов в какой-либо текст из позитивной или нейтральной группы, неизбежно появляются ошибки связанные с тем, что в оригинальном тексте и таргете, может в принципе говориться про разные продукты.\n",
    "\n",
    "Чтобы хоть как-то передать информацию о желаемым результатом, я предлагаю включить в промт информацию о продукте и типу текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['nmId', 'text', 'responder', 'type', 'product_name', 'product_category_2', 'product_category_1', 'product_color', 'product_description', 'product_brand', 'vector', 'toxicity', 'emotions', 'target_nmId', 'target_text', 'target_responder', 'target_type', 'target_product_name', 'target_product_category_2', 'target_product_category_1', 'target_product_color', 'target_product_description', 'target_product_brand', 'target_vector', 'target_toxicity', 'target_emotions'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.from_parquet('./__output__/mathced.parquet')\n",
    "ds = ds.select(np.random.randint(0, len(ds), 2000)) # type: ignore\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как у нас задача Seq2Seq модель, самым логичным вариантом будет выбрать какую-либо Encoder-Decoder модель, из которых самая популярная это flan-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    'google/flan-t5-large',\n",
    "    device_map='cuda:0',\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-small')\n",
    "\n",
    "coll = DataCollatorForSeq2Seq(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(e):\n",
    "    out = tokenizer(\n",
    "        f'''\n",
    "responder: {e['target_responder']}\n",
    "type: {e['target_type']}\n",
    "product_name: {e['target_product_name']}\n",
    "product_category_2: {e['target_product_category_2']}\n",
    "product_color: {e['target_product_color']}\n",
    "product_brand: {e['target_product_brand']}\n",
    "product_description: {e['target_product_description']}\n",
    "toxicity: {e['toxicity']}\n",
    "emotions: {e['emotions']}\n",
    "text: {e['text']}\n",
    "        ''',\n",
    "        truncation=True,\n",
    "        max_length=tokenizer.model_max_length\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        text_target=e['target_text'], \n",
    "        max_length=512,         \n",
    "        truncation=True\n",
    "    )\n",
    "    out['labels'] = labels['input_ids']\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4ff1b7d1df45dcab6dcffa4a4f402f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.map(preprocess, remove_columns=ds.column_names) # type: ignore\n",
    "ds = ds.train_test_split(test_size=0.1) # type: ignore\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fd81905c-e7e0-40e8-ae45-731658cd9780'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = str(uuid.uuid4())\n",
    "\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'./models/{checkpoint}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=f'./models/{checkpoint}/runs',\n",
    "    eval_strategy='epoch',\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=0.02,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    # fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roaoc\\AppData\\Local\\Temp\\ipykernel_6824\\2017768784.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, # type: ignore\n",
    "    args=args,\n",
    "    data_collator=coll,\n",
    "    train_dataset=ds['train'], # type: ignore\n",
    "    eval_dataset=ds['test'] # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e398c5c493914b829de7a31b3ed1dfcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f'./models/{checkpoint}/model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
